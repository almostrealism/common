# Spatial Module

The spatial module provides abstractions for converting time-series audio data into 3D spatial representations for visualization. It bridges frequency-domain audio analysis with 3D rendering systems and supports machine learning model integration.

## Purpose

This module exists to:

1. **Convert audio data to 3D visualizations** - Transform frequency spectra (spectrograms) into positioned points in 3D space
2. **Provide pub/sub coordination** - Manage communication between audio data producers and visualization consumers
3. **Support ML model outputs** - Represent audio generated by machine learning models with their associated parameters
4. **Enable hierarchical organization** - Organize audio recordings and model outputs in tree structures

## Core Concepts

### Spatial Hierarchy

The module defines a hierarchy of spatial objects:

```
Spatial (interface)
└── SpatialElement (base class with position)
    ├── SpatialGroup (container for grouping elements)
    └── SpatialValue<T> (positioned point with value and metadata)
```

- **`Spatial`** - Base interface for any object with a 3D position
- **`SpatialElement`** - Concrete positioned object
- **`SpatialGroup`** - Composite pattern for grouping elements
- **`SpatialValue`** - Data point with value, temperature (for color), and visualization hints

### Timeseries System

Time-varying data is represented through the `SpatialTimeseries` interface:

```
SpatialTimeseries (interface)
├── FrequencyTimeseries (abstract, converts frequency data to spatial)
│   └── FrequencyTimeseriesAdapter (delegation pattern)
│       ├── SpatialWaveDetails (wraps WaveDetails)
│       ├── GenomicTimeseries (ML model outputs with genome)
│       │   └── GenomicNetwork (neural network outputs)
│       └── SimpleTimeseries (tree-structured)
│           ├── RecordedTimeseries (recorded audio)
│           ├── AudioModelOutput (generative model output)
│           └── SoundDataTimeseries (bridges SoundData with visualization)
└── PlaceholderTimeseries (fallback when data unavailable)
```

### Coordinate Mapping

The `TemporalSpatialContext` class handles coordinate transformation:

- **X-axis**: Time (scaled by temporal scale and zoom)
- **Y-axis**: Frequency (in spatial frequency mode) or Channel
- **Z-axis**: Layer (for visual separation)

```java
TemporalSpatialContext context = new TemporalSpatialContext();
context.setDuration(10.0); // 10 seconds of audio

// Convert time/frequency to 3D position
Vector pos = context.position(time, channel, layer, frequency);
```

### Hub/Listener Pattern

Two hub classes coordinate event distribution:

**SpatialDataHub** - For timeseries data and selection events:
```java
SpatialDataHub hub = SpatialDataHub.getCurrent();
hub.addDataListener(data -> { /* handle data */ });
hub.addSelectionListener(info -> { /* handle selection */ });
hub.publish(new SpatialData<>(0, myTimeseries));
```

**SoundDataHub** - For audio playback coordination:
```java
SoundDataHub hub = SoundDataHub.getCurrent();
hub.addListener(listener);
hub.play();
hub.seek(30.0);
hub.pause();
```

## Key Classes

### Data Containers

| Class | Purpose |
|-------|---------|
| `SpatialData<T>` | Pairs a channel index with a timeseries |
| `SoundData` | Audio file reference with optional wave data |
| `SpatialValue<T>` | 3D point with value and visualization metadata |

### Timeseries Implementations

| Class | Purpose |
|-------|---------|
| `FrequencyTimeseries` | Abstract base for frequency-to-spatial conversion |
| `SpatialWaveDetails` | Wraps `WaveDetails` frequency analysis |
| `GenomicTimeseries` | ML-generated audio with genome parameters |
| `GenomicNetwork` | Neural network output with health score |
| `PlaceholderTimeseries` | Fallback visualization |

### Series Package (`com.almostrealism.spatial.series`)

| Class | Purpose |
|-------|---------|
| `SimpleTimeseries<T>` | Tree-structured base with delegation |
| `RecordedTimeseries` | Recorded audio (can be grouped) |
| `AudioModelOutput` | ML model output with conditions/embeddings |
| `SoundDataTimeseries` | Bridges `SoundData` with visualization |

### Listener Interfaces

| Interface | Purpose |
|-----------|---------|
| `SpatialDataListener` | Receives timeseries data and scan events |
| `SpatialSelectionListener` | Receives selection events |
| `SoundDataListener` | Receives audio playback events |

## Usage Examples

### Basic Frequency Visualization

```java
// Load wave details
WaveDetails details = AudioLibraryPersistence.loadWaveDetails("audio.bin");
SpatialWaveDetails spatial = new SpatialWaveDetails(details);

// Create visualization context
TemporalSpatialContext context = new TemporalSpatialContext();
context.setDuration(spatial.getDuration(context));

// Get spatial points
List<SpatialValue> elements = spatial.elements(context);
for (SpatialValue value : elements) {
    Vector pos = value.getPosition();
    double magnitude = value.getValue();
    double colorTemp = value.getTemperature();
    // Render point...
}
```

### ML Model Output Tracking

```java
// Create output representation
AudioModelOutput output = new AudioModelOutput("generated_001", "ambient synth pad");
output.setModelName("stable-audio");
output.setEmbed(embeddingVector);

// Access as PackedCollection for computation
PackedCollection embed = output.getPackedEmbed();
```

### Genomic Network Visualization

```java
GenomicNetwork network = new GenomicNetwork(0, genome);
network.setHealthScore(healthScore);

// Visualization automatically loads wave details or uses placeholder
List<SpatialValue> elements = network.elements(context);
```

### Sound Data Hub Integration

```java
SoundDataHub hub = SoundDataHub.getCurrent();

// Register for events
hub.addListener(new SoundDataListener() {
    @Override
    public void selected(SoundData d) {
        waveformView.loadAudio(d.getFile());
    }

    @Override
    public void published(int index, SoundData d) {
        channels.get(index).setSoundData(d);
    }

    @Override
    public void play() {
        audioPlayer.start();
    }
});

// Notify selection
hub.selected(new SoundData("/path/to/audio.wav"));
```

## Dependencies

This module depends on:

- **ar-audio-space** - Audio scene and wave data types
- **ar-remote** - Remote operation support

## Package Structure

```
com.almostrealism.spatial
├── Spatial.java                    # Base interface
├── SpatialElement.java             # Positioned object
├── SpatialGroup.java               # Element container
├── SpatialValue.java               # Data point with value
├── SpatialSource.java              # Element factory
├── SpatialTimeseries.java          # Timeseries interface
├── SpatialData.java                # Index + timeseries container
├── SpatialDataHub.java             # Timeseries pub/sub
├── SpatialDataListener.java        # Data event callback
├── SpatialSelectionListener.java   # Selection callback
├── SoundData.java                  # Audio file container
├── SoundDataHub.java               # Audio playback pub/sub
├── SoundDataListener.java          # Audio event callback
├── TemporalSpatialContext.java     # Coordinate mapping
├── FrequencyTimeseries.java        # Frequency visualization base
├── FrequencyTimeseriesAdapter.java # Delegation pattern
├── SpatialWaveDetails.java         # WaveDetails wrapper
├── PlaceholderTimeseries.java      # Fallback visualization
├── GenomicTimeseries.java          # ML output with genome
├── GenomicNetwork.java             # Neural network output
├── SpatialGenomic.java             # Genome marker interface
├── SpatialSurface.java             # Rendering integration
└── series/
    ├── SimpleTimeseries.java       # Tree-structured base
    ├── RecordedTimeseries.java     # Recorded audio
    ├── AudioModelOutput.java       # ML model output
    └── SoundDataTimeseries.java    # Sound/visualization bridge
```

## Architecture Notes

1. **Adapter Pattern**: `FrequencyTimeseriesAdapter` enables wrapping different data sources
2. **Observer Pattern**: Hubs coordinate communication between producers and consumers
3. **Composite Pattern**: `SpatialGroup` and tree-structured timeseries enable hierarchies
4. **Thread Safety**: `SoundDataHub.getCurrent()` returns an async-safe proxy

## Visualization Algorithm

The `FrequencyTimeseries.loadElements()` method converts frequency data to spatial points:

1. Iterates through time intervals
2. For each time position, scans all frequency bins
3. Creates `SpatialValue` for magnitudes above threshold (default: 35)
4. Applies logarithmic scaling: `Math.log(value + 1)`
5. Aggregates frequencies into quartiles for summary view
6. Retries with lower threshold if insufficient points

Threshold and scaling can be adjusted via:
```java
FrequencyTimeseries.frequencyThreshold = 35;  // Minimum magnitude
FrequencyTimeseries.frequencyScale = 1.0;     // Amplitude multiplier
```
