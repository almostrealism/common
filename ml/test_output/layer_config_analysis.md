# Layer Configuration Analysis Report

**Objective**: Identify what makes layers 2, 22, and 23 special

## Configuration Summary Table

| Layer | Status | QK-Norm | Q Bias | K Bias | V Bias | Q Std | Gate Std | Params |
|-------|--------|---------|--------|--------|--------|-------|----------|--------|
| 0 | Normal | No | Yes | Yes | Yes | 0.0667 | 0.0230 | 14912384 |
| 1 | Normal | No | Yes | Yes | Yes | 0.0287 | 0.0233 | 14912384 |
| 2 | **PROBLEM** | No | Yes | Yes | Yes | 0.0256 | 0.0237 | 14912384 |
| 3 | Normal | No | Yes | Yes | Yes | 0.0230 | 0.0226 | 14912384 |
| 4 | Normal | No | Yes | Yes | Yes | 0.0231 | 0.0225 | 14912384 |
| 5 | Normal | No | Yes | Yes | Yes | 0.0221 | 0.0243 | 14912384 |
| 6 | Normal | No | Yes | Yes | Yes | 0.0229 | 0.0220 | 14912384 |
| 7 | Normal | No | Yes | Yes | Yes | 0.0229 | 0.0212 | 14912384 |
| 8 | Normal | No | Yes | Yes | Yes | 0.0258 | 0.0217 | 14912384 |
| 9 | Normal | No | Yes | Yes | Yes | 0.0226 | 0.0205 | 14912384 |
| 10 | Normal | No | Yes | Yes | Yes | 0.0214 | 0.0213 | 14912384 |
| 11 | Normal | No | Yes | Yes | Yes | 0.0206 | 0.0194 | 14912384 |
| 12 | Normal | No | Yes | Yes | Yes | 0.0215 | 0.0200 | 14912384 |
| 13 | Normal | No | Yes | Yes | Yes | 0.0220 | 0.0194 | 14912384 |
| 14 | Normal | No | Yes | Yes | Yes | 0.0211 | 0.0201 | 14912384 |
| 15 | Normal | No | Yes | Yes | Yes | 0.0208 | 0.0200 | 14912384 |
| 16 | Normal | No | Yes | Yes | Yes | 0.0249 | 0.0213 | 14912384 |
| 17 | Normal | No | Yes | Yes | Yes | 0.0219 | 0.0222 | 14912384 |
| 18 | Normal | No | Yes | Yes | Yes | 0.0211 | 0.0212 | 14912384 |
| 19 | Normal | No | Yes | Yes | Yes | 0.0216 | 0.0211 | 14912384 |
| 20 | Normal | No | Yes | Yes | Yes | 0.0216 | 0.0208 | 14912384 |
| 21 | Normal | No | Yes | Yes | Yes | 0.0213 | 0.0207 | 14912384 |
| 22 | **PROBLEM** | No | Yes | Yes | Yes | 0.0212 | 0.0201 | 14912384 |
| 23 | **PROBLEM** | No | Yes | Yes | Yes | 0.0220 | 0.0206 | 14912384 |

## Weight Statistics Comparison

### Attention Weights (Mean ?? Std)

| Layer | Q Weight | K Weight | V Weight | O Weight |
|-------|----------|----------|----------|----------|
| **0** | -0.0000??0.0667 | 0.0001??0.0806 | -0.0000??0.0115 | 0.0000??0.0131 |
| **1** | 0.0001??0.0287 | 0.0000??0.0406 | 0.0001??0.0115 | -0.0000??0.0149 |
| **2** | 0.0000??0.0256 | 0.0001??0.0369 | 0.0000??0.0112 | -0.0000??0.0151 |
| **22** | -0.0000??0.0212 | -0.0000??0.0184 | 0.0000??0.0275 | 0.0000??0.0216 |
| **23** | -0.0000??0.0220 | 0.0000??0.0188 | 0.0001??0.0254 | 0.0000??0.0204 |

### FFN Weights (Mean ?? Std)

| Layer | Gate Weight | Up Weight | Down Weight |
|-------|-------------|-----------|-------------|
| **0** | -0.0000??0.0230 | -0.0000??0.0183 | -0.0000??0.0182 |
| **1** | 0.0001??0.0233 | 0.0000??0.0170 | 0.0000??0.0170 |
| **2** | 0.0000??0.0237 | -0.0000??0.0173 | 0.0000??0.0170 |
| **22** | 0.0001??0.0201 | -0.0000??0.0208 | 0.0000??0.0200 |
| **23** | 0.0001??0.0206 | 0.0000??0.0204 | -0.0000??0.0179 |

## Key Findings

- **QK-Norm Distribution**: Not all layers have QK-Norm weights
- **Bias Distribution**: All layers have Q/K/V biases (uniform)

- **Q Weight Variance Range**: 0.0206 (layer 11) to 0.0667 (layer 0)

## Recommendations

Based on this analysis:
1. Check if weight initialization differs for problematic layers
2. Verify normalization implementation for edge cases
3. Test attention mechanism with extreme weight values
4. Check for numerical overflow/underflow in FFN

---
*Generated by LayerConfigurationTest*
