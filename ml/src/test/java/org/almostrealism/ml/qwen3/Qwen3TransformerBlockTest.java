package org.almostrealism.ml.qwen3;

import io.almostrealism.collect.TraversalPolicy;
import org.almostrealism.collect.PackedCollection;
import org.almostrealism.layers.LayerFeatures;
import org.almostrealism.ml.AttentionFeatures;
import org.almostrealism.ml.StateDictionary;
import org.almostrealism.model.Block;
import org.almostrealism.model.CompiledModel;
import org.almostrealism.model.Model;
import org.almostrealism.model.SequentialBlock;
import org.almostrealism.util.TestFeatures;
import org.almostrealism.util.TestUtils;
import org.junit.Test;

import static org.junit.Assert.*;

/**
 * Validates Qwen3 transformer block implementation against PyTorch reference.
 *
 * This test loads reference data generated by generate_qwen3_reference.py which
 * runs a real Qwen2.5-0.5B-Instruct transformer layer through PyTorch and saves
 * the inputs, weights, and expected outputs.
 */
public class Qwen3TransformerBlockTest implements AttentionFeatures, LayerFeatures, TestFeatures {

	@Test
	public void testTransformerBlockReference() throws Exception {
		if (testProfileIs(TestUtils.PIPELINE)) return;

		String referenceDir = "/workspace/project/common/ml/qwen3_reference/qwen3_transformer_block";

		// Load reference data
		System.out.println("Loading reference data from: " + referenceDir);
		StateDictionary referenceData = new StateDictionary(referenceDir);

		System.out.println("Reference data keys:");
		referenceData.keySet()
				.forEach(key -> System.out.println("\t" + key + " " + referenceData.get(key).getShape()));

		// Extract test configuration
		PackedCollection testConfig = referenceData.get("test_config");
		int batchSize = (int) testConfig.valueAt(0);
		int seqLen = (int) testConfig.valueAt(1);
		int dim = (int) testConfig.valueAt(2);
		int hiddenDim = (int) testConfig.valueAt(3);
		int heads = (int) testConfig.valueAt(4);
		int kvHeads = (int) testConfig.valueAt(5);
		int headSize = (int) testConfig.valueAt(6);

		System.out.println("\nTransformer block test configuration:");
		System.out.println("  batchSize=" + batchSize + ", seqLen=" + seqLen + ", dim=" + dim);
		System.out.println("  hiddenDim=" + hiddenDim + ", heads=" + heads + ", kvHeads=" + kvHeads + ", headSize=" + headSize);

		// Load test data
		PackedCollection input = referenceData.get("input");
		PackedCollection expectedOutput = referenceData.get("expected_output");

		// Load position embeddings
		PackedCollection positionIds = referenceData.get("position_ids");
		PackedCollection posCos = referenceData.get("position_cos");
		PackedCollection posSin = referenceData.get("position_sin");

		// Load attention weights
		// Note: PyTorch and AR both use (out, in) format for dense layers - no transpose needed
		PackedCollection wq = referenceData.get("self_attn.q_proj.weight");
		PackedCollection wk = referenceData.get("self_attn.k_proj.weight");
		PackedCollection wv = referenceData.get("self_attn.v_proj.weight");
		PackedCollection wo = referenceData.get("self_attn.o_proj.weight");

		// Load attention biases (Q/K/V have biases, O does not)
		PackedCollection bq = referenceData.get("self_attn.q_proj.bias");
		PackedCollection bk = referenceData.get("self_attn.k_proj.bias");
		PackedCollection bv = referenceData.get("self_attn.v_proj.bias");

		// Load normalization weights (1D, no transpose needed)
		PackedCollection attnNorm = referenceData.get("input_layernorm.weight");
		PackedCollection ffnNorm = referenceData.get("post_attention_layernorm.weight");

		// Load FFN weights (same (out, in) format - no transpose needed)
		PackedCollection wGate = referenceData.get("mlp.gate_proj.weight");
		PackedCollection wUp = referenceData.get("mlp.up_proj.weight");
		PackedCollection wDown = referenceData.get("mlp.down_proj.weight");

		System.out.println("\nWeight shapes:");
		System.out.println("  wq: " + wq.getShape());
		System.out.println("  wk: " + wk.getShape());
		System.out.println("  wv: " + wv.getShape());
		System.out.println("  wo: " + wo.getShape());
		System.out.println("  wGate: " + wGate.getShape());
		System.out.println("  wUp: " + wUp.getShape());
		System.out.println("  wDown: " + wDown.getShape());

		// Build transformer block using our implementation
		// Model expects (dim) for single-token processing
		// We'll process the full sequence by iterating
		Model model = new Model(shape(dim));
		SequentialBlock main = model.sequential();

		// Note: PyTorch transformer output has shape (batch, seq, dim) = (1, 8, 896)
		// Our implementation processes sequences iteratively

		System.out.println("\nBuilding transformer block...");

		// Compute RoPE frequencies
		// Qwen uses theta=1000000.0 for extended context
		PackedCollection freqCis = computeRopeFreqs(seqLen, headSize, 1000000.0);
		System.out.println("  RoPE freqs shape: " + freqCis.getShape());

		// Position placeholder (will test position 0)
		PackedCollection position = new PackedCollection(1);
		position.setMem(0, 0.0);  // Test first position

		// Build full transformer block
		// Note: Qwen2.5-0.5B doesn't have QK-Norm, so pass null
		main.add(transformer(heads, kvHeads,
				attnNorm,              // Attention pre-norm
				wk, wv, wq, wo,       // Attention weights
				bk, bv, bq,            // Attention biases (K, V, Q)
				null, null,            // No QK-Norm in this model
				freqCis,               // RoPE frequencies
				ffnNorm,               // FFN pre-norm
				wGate, wDown, wUp,    // FFN weights
				p(position)));

		System.out.println("  Transformer block built successfully");
		System.out.println("\nCompiling model...");
		CompiledModel compiled = model.compile(false);

		// Extract first token from input (batch=0, position=0)
		PackedCollection firstToken = new PackedCollection(shape(dim));
		for (int d = 0; d < dim; d++) {
			firstToken.setMem(d, input.valueAt(0, 0, d));
		}

		// Check input is reasonable
		System.out.println("\nInput diagnostics:");
		System.out.println("  First 5 values: " + firstToken.valueAt(0) + ", " + firstToken.valueAt(1) + ", " + firstToken.valueAt(2));
		System.out.println("  Sum: " + firstToken.doubleStream().sum());
		double inputMean = firstToken.doubleStream().average().orElse(0);
		double inputMax = firstToken.doubleStream().map(Math::abs).max().orElse(0);
		System.out.println("  Mean: " + inputMean + ", Max abs: " + inputMax);

		System.out.println("\nRunning forward pass on first token...");
		PackedCollection rawOutput = compiled.forward(firstToken);

		System.out.println("  Raw output shape: " + rawOutput.getShape());
		System.out.println("  Raw output memLength: " + rawOutput.getMemLength());
		if (rawOutput.getMemLength() > 0) {
			// Output is 2D (1, 896), so use 2D indexing
			System.out.println("  Raw output first 5 values: " + rawOutput.valueAt(0, 0) + ", " + rawOutput.valueAt(0, 1) + ", " + rawOutput.valueAt(0, 2));
			System.out.println("  Raw output sum: " + rawOutput.doubleStream().sum());
			boolean hasNaN = rawOutput.doubleStream().anyMatch(Double::isNaN);
			boolean hasInf = rawOutput.doubleStream().anyMatch(Double::isInfinite);
			System.out.println("  Has NaN: " + hasNaN + ", Has Inf: " + hasInf);
		} else {
			System.out.println("  WARNING: Raw output is empty!");
		}

		// Squeeze output if needed (remove batch dimension)
		PackedCollection actualOutput;
		if (rawOutput.getShape().getDimensions() == 2 && rawOutput.getShape().length(0) == 1) {
			// Squeeze (1, dim) -> (dim)
			actualOutput = new PackedCollection(shape(dim));
			for (int d = 0; d < dim; d++) {
				actualOutput.setMem(d, rawOutput.valueAt(0, d));
			}
		} else {
			actualOutput = rawOutput;
		}

		// Compare with PyTorch output for first token
		PackedCollection expectedFirstToken = new PackedCollection(shape(dim));
		for (int d = 0; d < dim; d++) {
			expectedFirstToken.setMem(d, expectedOutput.valueAt(0, 0, d));
		}

		System.out.println("\nOutput comparison (first token, position 0):");
		System.out.println("  Expected shape: " + expectedFirstToken.getShape());
		System.out.println("  Actual shape: " + actualOutput.getShape());
		System.out.println("  Expected sum: " + expectedFirstToken.doubleStream().sum());
		System.out.println("  Actual sum: " + actualOutput.doubleStream().sum());

		// Compare outputs
		double maxDiff = compare(expectedFirstToken, actualOutput);
		System.out.println("  Max absolute difference: " + maxDiff);

		// Assert similarity (use relaxed tolerance for initial test)
		double tolerance = 0.1;  // 10% tolerance
		assertTrue("Transformer block output differs from PyTorch by " + maxDiff +
				" (tolerance: " + tolerance + ")", maxDiff < tolerance);

		System.out.println("\n[OK] Transformer block validation passed!");
		System.out.println("Next steps:");
		System.out.println("  1. Test all sequence positions");
		System.out.println("  2. Tighten tolerance if passing");
		System.out.println("  3. If failing, isolate failing component");

		// Clean up
		referenceData.destroy();
	}


	/**
	 * Compute RoPE frequency matrix for positional embeddings.
	 *
	 * @param seqLen Sequence length
	 * @param headSize Dimension of each attention head
	 * @param theta RoPE theta parameter (10000.0 for Llama, 1000000.0 for Qwen)
	 * @return Frequency matrix of shape (seqLen, headSize/2, 2) for complex cos/sin pairs
	 */
	private PackedCollection computeRopeFreqs(int seqLen, int headSize, double theta) {
		// RoPE applies to half the head dimensions (complex number representation)
		int freqDim = headSize / 2;

		// Compute inverse frequencies
		PackedCollection invFreq = new PackedCollection(shape(freqDim));
		for (int i = 0; i < freqDim; i++) {
			invFreq.setMem(i, 1.0 / Math.pow(theta, (2.0 * i) / headSize));
		}

		// Compute position * inv_freq for each position
		// Shape: (seqLen, freqDim, 2) where freqDim = headSize/2
		PackedCollection freqs = new PackedCollection(shape(seqLen, freqDim, 2));
		for (int pos = 0; pos < seqLen; pos++) {
			for (int i = 0; i < freqDim; i++) {
				double freq = pos * invFreq.toDouble(i);
				// Store as interleaved cos/sin pairs: [cos0, sin0, cos1, sin1, ...]
				int idx = (pos * freqDim + i) * 2;
				freqs.setMem(idx, Math.cos(freq));
				freqs.setMem(idx + 1, Math.sin(freq));
			}
		}

		return freqs;
	}

	/**
	 * Compare two tensors and return maximum absolute difference.
	 */
	public double compare(PackedCollection expected, PackedCollection actual) {
		double maxDiff = 0.0;
		int count = expected.getMemLength();

		for (int i = 0; i < count; i++) {
			double diff = Math.abs(expected.valueAt(i) - actual.valueAt(i));
			maxDiff = Math.max(maxDiff, diff);
		}

		return maxDiff;
	}
}
