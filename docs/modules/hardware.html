<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AR-Hardware Module - Almost Realism Framework</title>
    <link rel="stylesheet" href="../css/style.css">
    <style>
        .key-concepts { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
        .concept-card { background: #f8f9fa; border-left: 4px solid #1976d2; padding: 15px; border-radius: 4px; }
        .concept-card h4 { margin-top: 0; color: #1976d2; }
        .performance-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        .performance-table th, .performance-table td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
        .performance-table th { background-color: #e3f2fd; color: #1976d2; font-weight: bold; }
        .performance-table tr:hover { background-color: #f5f5f5; }
        .comparison-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        .comparison-table th, .comparison-table td { padding: 10px; border: 1px solid #ddd; text-align: left; }
        .comparison-table th { background-color: #e3f2fd; color: #1976d2; }
        .class-list { display: grid; grid-template-columns: repeat(auto-fill, minmax(250px, 1fr)); gap: 15px; margin: 20px 0; }
        .class-item { background: #fff; border: 1px solid #ddd; padding: 12px; border-radius: 4px; }
        .class-item h4 { margin: 0 0 8px 0; color: #1976d2; font-size: 1.1em; }
        .class-item p { margin: 0; font-size: 0.9em; color: #666; }
        .backend-card { background: #fff; border: 2px solid #1976d2; padding: 15px; margin: 15px 0; border-radius: 4px; }
        .backend-card h4 { margin-top: 0; color: #1976d2; }
        .code-example { background: #f5f5f5; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0; border-radius: 4px; }
        .highlight { background-color: #fff9c4; padding: 2px 4px; }
        .warning { background-color: #ffebee; border-left: 4px solid #c62828; padding: 12px; margin: 15px 0; }
        .success { background-color: #e8f5e9; border-left: 4px solid #2e7d32; padding: 12px; margin: 15px 0; }
        .env-var { font-family: monospace; background: #e3f2fd; padding: 2px 6px; border-radius: 3px; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>AR-Hardware Module</h1>
            <p class="tagline">Multi-Backend Hardware Acceleration for CPU, GPU, OpenCL & Metal</p>
            <nav>
                <a href="../index.html">Home</a>
                <a href="#overview">Overview</a>
                <a href="#architecture">Architecture</a>
                <a href="#backends">Backends</a>
                <a href="#classes">Key Classes</a>
                <a href="#performance">Performance</a>
                <a href="#examples">Examples</a>
            </nav>
        </header>

        <main>
            <section id="overview">
                <h2>Overview</h2>
                <p>The <strong>ar-hardware</strong> module is the foundational layer for hardware-accelerated computation in Almost Realism. It provides a unified abstraction for compiling and executing computational graphs on various hardware backends including CPU (JNI), GPU (OpenCL), and Apple Silicon (Metal).</p>

                <div class="key-concepts">
                    <div class="concept-card">
                        <h4>üöÄ Hardware Acceleration</h4>
                        <p>Execute computations on CPU, GPU, or specialized accelerators. Automatic compilation to OpenCL, Metal, or native C code with kernel caching for 10,000√ó speedup.</p>
                    </div>
                    <div class="concept-card">
                        <h4>üíæ Memory Abstraction</h4>
                        <p>Unified memory interface across heap, off-heap, and device memory. Automatic transfers between providers, zero-copy delegation, and GC-integrated native memory.</p>
                    </div>
                    <div class="concept-card">
                        <h4>üì¶ Multi-Backend</h4>
                        <p>OpenCL for cross-platform GPU, Metal for Apple Silicon, JNI for native C execution. Automatic backend selection with fallback support.</p>
                    </div>
                    <div class="concept-card">
                        <h4>üîß Zero Configuration</h4>
                        <p>Set two environment variables and go. No code changes required to switch backends or precision modes.</p>
                    </div>
                </div>
            </section>

            <section id="architecture">
                <h2>Module Architecture</h2>

                <h3>Core Abstractions</h3>
                <div class="class-list">
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/Hardware.html">Hardware</a></h4>
                        <p>Global configuration and initialization. Entry point for accessing compute contexts and memory providers.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/HardwareOperator.html">HardwareOperator</a></h4>
                        <p>Base class for compiled kernel execution. Manages argument preparation, memory migration, and work size configuration.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/DefaultComputer.html">DefaultComputer</a></h4>
                        <p>Compilation and caching coordinator. Thread-local compute requirements stack with signature-based kernel caching.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/MemoryData.html">MemoryData</a></h4>
                        <p>Hardware-accessible data abstraction. Unified interface for heap, off-heap, and device memory with traversal policies.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/MemoryBank.html">MemoryBank</a></h4>
                        <p>Collection of <a href="../apidocs/org/almostrealism/hardware/MemoryData.html">MemoryData</a> in single allocation. Enables batch operations with single GPU transfer.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/PassThroughProducer.html">PassThroughProducer</a></h4>
                        <p>Dynamic input placeholders. Enables kernel reuse with different data at evaluation time.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a></h4>
                        <p>Composable operation sequences. Can compile multiple operations into single kernel or execute sequentially.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/AcceleratedOperation.html">AcceleratedOperation</a></h4>
                        <p>Hardware-accelerated computation base. Compiles Scope to backend-specific code (OpenCL C, MSL, JNI C).</p>
                    </div>
                </div>

                <h3>Memory Management</h3>
                <div class="class-list">
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/mem/MemoryProvider.html">MemoryProvider</a></h4>
                        <p>Allocates and manages hardware memory. Provider-specific implementations for OpenCL, Metal, and heap memory.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/mem/Heap.html">Heap</a></h4>
                        <p>Thread-local arena allocation. Efficient staging for temporary memory with automatic cleanup.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/mem/RAM.html">RAM</a></h4>
                        <p>Base class for native memory. Provides pointer-based access to off-heap allocations.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/mem/Bytes.html">Bytes</a></h4>
                        <p>Fixed-size memory block. Fundamental building block with delegation support for zero-copy views.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/mem/MemoryDataAdapter.html">MemoryDataAdapter</a></h4>
                        <p>Delegation and versioning. Creates zero-copy views and caches memory across providers.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/mem/MemoryReplacementManager.html">MemoryReplacementManager</a></h4>
                        <p>Three-phase memory substitution. Handles CPU‚ÜíGPU transfers with aggregation and root grouping.</p>
                    </div>
                </div>

                <h3>Context Management</h3>
                <div class="class-list">
                    <div class="class-item">
                        <h4><a href="../apidocs/io/almostrealism/code/DataContext.html">DataContext</a></h4>
                        <p>Backend-specific data management. Provides <a href="../apidocs/org/almostrealism/hardware/mem/MemoryProvider.html">MemoryProvider</a> and <a href="../apidocs/io/almostrealism/code/ComputeContext.html">ComputeContext</a> for a hardware backend.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/io/almostrealism/code/ComputeContext.html">ComputeContext</a></h4>
                        <p>Kernel compilation and execution. Converts Scope to backend code and manages instruction caching.</p>
                    </div>
                    <div class="class-item">
                        <h4><a href="../apidocs/org/almostrealism/hardware/ctx/ContextSpecific.html">ContextSpecific</a></h4>
                        <p>Thread-local context binding. Ensures operations use correct backend context.</p>
                    </div>
                </div>
            </section>

            <section id="backends">
                <h2>Hardware Backends</h2>

                <div class="backend-card">
                    <h4>üñ•Ô∏è OpenCL Backend (<a href="../apidocs/org/almostrealism/hardware/cl/package-summary.html">cl package</a> - 21 classes)</h4>
                    <p><strong>Purpose:</strong> Cross-platform GPU/CPU acceleration via OpenCL 1.2+</p>
                    <p><strong>Key Classes:</strong>
                        <a href="../apidocs/org/almostrealism/hardware/cl/CLDataContext.html">CLDataContext</a>,
                        <a href="../apidocs/org/almostrealism/hardware/cl/CLMemoryProvider.html">CLMemoryProvider</a>,
                        <a href="../apidocs/org/almostrealism/hardware/cl/CLOperator.html">CLOperator</a>,
                        <a href="../apidocs/org/almostrealism/hardware/cl/CLProgram.html">CLProgram</a>
                    </p>
                    <p><strong>Use Case:</strong> General-purpose GPU acceleration on NVIDIA, AMD, Intel GPUs</p>
                    <p><strong>Configuration:</strong> <span class="env-var">AR_HARDWARE_DRIVER=cl</span> or <span class="env-var">gpu</span></p>
                    <ul>
                        <li>Supports CPU fallback mode</li>
                        <li>Multiple allocation strategies (DEVICE, HOST, HEAP, DELEGATE)</li>
                        <li>JNI-based native compilation option (<a href="../apidocs/org/almostrealism/hardware/cl/CLJNIPrintWriter.html">CLJNIPrintWriter</a>)</li>
                        <li>Exception mapping for CL_INVALID_CONTEXT, CL_INVALID_VALUE</li>
                    </ul>
                </div>

                <div class="backend-card">
                    <h4>üçé Metal Backend (<a href="../apidocs/org/almostrealism/hardware/metal/package-summary.html">metal package</a> - 21 classes)</h4>
                    <p><strong>Purpose:</strong> Apple Silicon GPU acceleration via Metal Shading Language (MSL)</p>

                    <p><strong>Core Metal Wrappers (JNI via libMTL.dylib):</strong></p>
                    <ul>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLDevice.html">MTLDevice</a> - GPU device abstraction (id&lt;MTLDevice&gt;)</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLBuffer.html">MTLBuffer</a> - GPU memory buffers with FP16/FP32 support</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandQueue.html">MTLCommandQueue</a> - Command submission queue</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandBuffer.html">MTLCommandBuffer</a> - One-time-use command buffer</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLComputeCommandEncoder.html">MTLComputeCommandEncoder</a> - Command encoding API</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLComputePipelineState.html">MTLComputePipelineState</a> - Compiled compute pipeline</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MTLFunction.html">MTLFunction</a> - Compiled MSL function wrapper</li>
                    </ul>

                    <p><strong>High-Level Abstractions:</strong></p>
                    <ul>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalDataContext.html">MetalDataContext</a> - Backend initialization and device management</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalComputeContext.html">MetalComputeContext</a> - MSL compilation and program caching</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalMemoryProvider.html">MetalMemoryProvider</a> - MTLBuffer allocation with reference counting</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalMemory.html">MetalMemory</a> - RAM wrapper backed by MTLBuffer</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a> - Kernel execution with argument encoding</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalOperatorMap.html">MetalOperatorMap</a> - Thread-local operator management</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalProgram.html">MetalProgram</a> - MSL source compilation wrapper</li>
                    </ul>

                    <p><strong>Code Generation:</strong></p>
                    <ul>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalLanguageOperations.html">MetalLanguageOperations</a> - MSL-specific syntax (address spaces, kernel index)</li>
                        <li><a href="../apidocs/org/almostrealism/hardware/metal/MetalPrintWriter.html">MetalPrintWriter</a> - MSL code output with [[kernel]] attributes</li>
                    </ul>

                    <p><strong>Use Case:</strong> High-performance computation on macOS 10.13+/iOS 11+ with M1/M2/M3 or Intel Iris Plus GPUs</p>
                    <p><strong>Configuration:</strong> <span class="env-var">AR_HARDWARE_DRIVER=mtl</span> or <span class="env-var">gpu</span></p>

                    <p><strong>Metal-Specific Features:</strong></p>
                    <ul>
                        <li><strong>Unified Memory Architecture:</strong> Shared memory mode eliminates CPU‚ÜîGPU transfers on Apple Silicon</li>
                        <li><strong>Precision Support:</strong> Native FP16 and FP32 operations</li>
                        <li><strong>Threadgroup Optimization:</strong> Automatic SIMD width detection (typically 32 threads)</li>
                        <li><strong>Dispatch Methods:</strong> dispatchThreads (total threads) vs dispatchThreadgroups (explicit control)</li>
                        <li><strong>Reference Counting:</strong> GC-integrated MTLBuffer lifecycle via <a href="../apidocs/org/almostrealism/hardware/metal/MetalMemoryRef.html">MetalMemoryRef</a></li>
                        <li><strong>Thread-Safe Execution:</strong> Thread-local <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a> instances</li>
                    </ul>
                </div>

                <div class="backend-card">
                    <h4>‚ö° JNI Backend (<a href="../apidocs/org/almostrealism/hardware/jni/package-summary.html">jni package</a> - 15 classes)</h4>
                    <p><strong>Purpose:</strong> Fast CPU execution via runtime-compiled native C code</p>
                    <p><strong>Key Classes:</strong>
                        <a href="../apidocs/org/almostrealism/hardware/jni/NativeCompiler.html">NativeCompiler</a>,
                        <a href="../apidocs/org/almostrealism/hardware/jni/NativeExecution.html">NativeExecution</a>,
                        <a href="../apidocs/org/almostrealism/hardware/jni/NativeDataContext.html">NativeDataContext</a>
                    </p>
                    <p><strong>Use Case:</strong> CPU-bound workloads, development, systems without GPU support</p>
                    <p><strong>Configuration:</strong> <span class="env-var">AR_HARDWARE_DRIVER=native</span> or <span class="env-var">cpu</span></p>
                    <ul>
                        <li>Dynamic C code generation and compilation</li>
                        <li>GCC/Clang support with optimization flags</li>
                        <li>Thread-safe library loading</li>
                        <li>Stack-allocated data for small operations</li>
                    </ul>
                </div>

                <h3>Backend Selection Strategy</h3>
                <table class="comparison-table">
                    <tr>
                        <th>Backend</th>
                        <th>Best For</th>
                        <th>Precision</th>
                        <th>Platform</th>
                    </tr>
                    <tr>
                        <td><strong>Metal</strong></td>
                        <td>Apple Silicon, iOS apps, unified memory</td>
                        <td>FP16/FP32</td>
                        <td>macOS 10.13+, iOS 11+</td>
                    </tr>
                    <tr>
                        <td><strong>OpenCL</strong></td>
                        <td>Cross-platform GPU, NVIDIA/AMD cards</td>
                        <td>FP32/FP64</td>
                        <td>Windows, Linux, macOS</td>
                    </tr>
                    <tr>
                        <td><strong>JNI (Native)</strong></td>
                        <td>CPU workloads, development, debugging</td>
                        <td>FP32/FP64</td>
                        <td>Any platform with C compiler</td>
                    </tr>
                </table>
            </section>

            <section id="classes">
                <h2>Key Classes Deep Dive</h2>

                <h3><a href="../apidocs/org/almostrealism/hardware/Hardware.html">Hardware</a></h3>
                <p>Global configuration singleton managing backend selection and initialization.</p>
                <ul>
                    <li><strong>Backend Selection:</strong> Environment-driven (AR_HARDWARE_DRIVER)</li>
                    <li><strong>DataContext Factory:</strong> Creates <a href="../apidocs/org/almostrealism/hardware/cl/CLDataContext.html">CLDataContext</a>, <a href="../apidocs/org/almostrealism/hardware/metal/MetalDataContext.html">MetalDataContext</a>, or <a href="../apidocs/org/almostrealism/hardware/jni/NativeDataContext.html">NativeDataContext</a></li>
                    <li><strong>Memory Scale:</strong> Configures max memory via AR_HARDWARE_MEMORY_SCALE</li>
                    <li><strong>Precision:</strong> FP32 vs FP64 via AR_HARDWARE_PRECISION</li>
                    <li><strong>Thread-Local Computer:</strong> Per-thread <a href="../apidocs/org/almostrealism/hardware/DefaultComputer.html">DefaultComputer</a> instances</li>
                </ul>

                <h3><a href="../apidocs/org/almostrealism/hardware/DefaultComputer.html">DefaultComputer</a></h3>
                <p>Compilation coordinator with multi-level caching and compute requirements management.</p>
                <ul>
                    <li><strong>Requirements Stack:</strong> Thread-local stack of ComputeRequirement (CPU/GPU/AUTO)</li>
                    <li><strong>Signature Caching:</strong> Cache by operation signature hash</li>
                    <li><strong>Provider Tracking:</strong> Monitors last-used <a href="../apidocs/org/almostrealism/hardware/mem/MemoryProvider.html">MemoryProvider</a> per operation</li>
                    <li><strong>Lazy Compilation:</strong> Defers kernel compilation until first evaluation</li>
                </ul>

                <h3><a href="../apidocs/org/almostrealism/hardware/PassThroughProducer.html">PassThroughProducer</a></h3>
                <p>Represents dynamic input arguments for kernel reuse.</p>
                <ul>
                    <li><strong>Argument Index:</strong> References evaluation argument by position (0, 1, 2, ...)</li>
                    <li><strong>Traversal Policy:</strong> Supports fixed and variable-count policies</li>
                    <li><strong>Kernel Reuse:</strong> Same kernel works with different data</li>
                    <li><strong>Memory Destination:</strong> Optional <a href="../apidocs/org/almostrealism/hardware/mem/MemoryDataDestinationProducer.html">MemoryDataDestinationProducer</a> for pre-allocated outputs</li>
                </ul>

                <h3><a href="../apidocs/org/almostrealism/hardware/MemoryBank.html">MemoryBank</a></h3>
                <p>Collection of <a href="../apidocs/org/almostrealism/hardware/MemoryData.html">MemoryData</a> elements in a single contiguous allocation.</p>
                <ul>
                    <li><strong>Single GPU Transfer:</strong> Entire bank transfers as one operation</li>
                    <li><strong>Factory Methods:</strong> Vector.bank(n), <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a>.bank(count, size)</li>
                    <li><strong>Get/Set:</strong> Access individual elements via index</li>
                    <li><strong>Delegation:</strong> Each element is a zero-copy view into bank</li>
                </ul>

                <h3><a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a></h3>
                <p>Composable sequence of operations with compilation support.</p>
                <ul>
                    <li><strong>Compilation Criteria:</strong> All elements must be <a href="../apidocs/io/almostrealism/code/Computation.html">Computation</a>, depth ‚â§ maxDepth</li>
                    <li><strong>Dual Execution:</strong> Compiled (single kernel) or Sequential (multiple kernels)</li>
                    <li><strong>Compute Requirements:</strong> Per-list GPU/CPU preferences</li>
                    <li><strong>Flattening:</strong> Nested lists can be flattened to reduce depth</li>
                    <li><strong>Isolation:</strong> Optional operation isolation for debugging</li>
                </ul>

                <h3><a href="../apidocs/org/almostrealism/hardware/mem/Heap.html">Heap</a></h3>
                <p>Thread-local arena allocator for efficient temporary memory.</p>
                <ul>
                    <li><strong>Staged Allocation:</strong> Nested stages with automatic cleanup</li>
                    <li><strong>Thread-Local Default:</strong> <code>Heap.getDefault()</code> per thread</li>
                    <li><strong>Scope-Based:</strong> <code>heap.use(() -> {...})</code> for automatic management</li>
                    <li><strong>Performance:</strong> Single large allocation instead of many small ones</li>
                </ul>

                <h3>Metal Backend Architecture</h3>
                <p>The Metal backend provides comprehensive GPU acceleration for Apple platforms through a carefully designed JNI bridge and high-level abstractions.</p>

                <h4>Command Encoding Workflow</h4>
                <p>Metal uses a command encoding pattern for GPU operations:</p>
                <ol>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandQueue.html">MTLCommandQueue</a>.commandBuffer()</strong> - Create one-time-use command buffer</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandBuffer.html">MTLCommandBuffer</a>.encoder()</strong> - Begin encoding compute commands</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLComputeCommandEncoder.html">MTLComputeCommandEncoder</a>.setComputePipelineState()</strong> - Bind compiled kernel</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLComputeCommandEncoder.html">MTLComputeCommandEncoder</a>.setBuffer()</strong> - Bind argument buffers</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLComputeCommandEncoder.html">MTLComputeCommandEncoder</a>.dispatchThreads()</strong> - Configure grid size</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLComputeCommandEncoder.html">MTLComputeCommandEncoder</a>.endEncoding()</strong> - Finalize encoding</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandBuffer.html">MTLCommandBuffer</a>.commit()</strong> - Submit to GPU</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandBuffer.html">MTLCommandBuffer</a>.waitUntilCompleted()</strong> - Synchronize with GPU</li>
                </ol>

                <h4>Memory Management</h4>
                <ul>
                    <li><strong>Storage Modes:</strong> Shared (unified memory, zero-copy on Apple Silicon) vs Managed (explicit synchronization)</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MetalMemoryProvider.html">MetalMemoryProvider</a>:</strong> Allocates MTLBuffers and tracks them with <a href="../apidocs/org/almostrealism/hardware/metal/MetalMemoryRef.html">MetalMemoryRef</a></li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MetalMemoryRef.html">MetalMemoryRef</a>:</strong> PhantomReference-based GC tracking for automatic MTLBuffer deallocation</li>
                    <li><strong>Reference Counting:</strong> Native Metal objects use release() pattern via <a href="../apidocs/org/almostrealism/hardware/metal/MTLObject.html">MTLObject</a> base class</li>
                </ul>

                <h4>MSL Code Generation</h4>
                <ul>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MetalPrintWriter.html">MetalPrintWriter</a>:</strong> Configures kernel prefix as <code>[[kernel]] void</code> for entry points</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MetalLanguageOperations.html">MetalLanguageOperations</a>:</strong> Metal-specific syntax (device/thread address spaces, [[buffer(N)]] attributes)</li>
                    <li><strong>Kernel Attributes:</strong> [[kernel]], [[buffer(N)]], [[thread_position_in_grid]]</li>
                    <li><strong>Address Spaces:</strong> <code>device</code> for GPU memory, <code>thread</code> for private memory</li>
                </ul>

                <h4>Threadgroup Sizing</h4>
                <ul>
                    <li><strong>SIMD Width:</strong> Query via <a href="../apidocs/org/almostrealism/hardware/metal/MTLComputePipelineState.html">MTLComputePipelineState</a>.threadExecutionWidth() (typically 32)</li>
                    <li><strong>Max Threads:</strong> Query via maxTotalThreadsPerThreadgroup() (typically 1024 on Apple Silicon)</li>
                    <li><strong>Dispatch Modes:</strong>
                        <ul>
                            <li><code>dispatchThreads(totalX, totalY, totalZ, groupX, groupY, groupZ)</code> - Specify total threads + threadgroup size</li>
                            <li><code>dispatchThreadgroups(groupW, groupH, groupD, gridW, gridH, gridD)</code> - Explicit threadgroup count</li>
                        </ul>
                    </li>
                </ul>

                <h4>Thread Safety</h4>
                <ul>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MetalOperatorMap.html">MetalOperatorMap</a>:</strong> Thread-local <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a> instances</li>
                    <li><strong><a href="../apidocs/org/almostrealism/hardware/metal/MetalCommandRunner.html">MetalCommandRunner</a>:</strong> Single-threaded executor service for command submission</li>
                    <li><strong>Shared Program:</strong> <a href="../apidocs/org/almostrealism/hardware/metal/MetalProgram.html">MetalProgram</a> is immutable after compilation, shared across threads</li>
                </ul>
            </section>

            <section id="performance">
                <h2>Performance Characteristics</h2>

                <h3>Kernel Compilation Overhead</h3>
                <table class="performance-table">
                    <thead>
                        <tr>
                            <th>Operation</th>
                            <th>First Call (Compile + Execute)</th>
                            <th>Subsequent Calls (Execute Only)</th>
                            <th>Speedup</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Simple multiply (1000 elements)</td>
                            <td>100ms</td>
                            <td>0.01ms</td>
                            <td>10,000√ó</td>
                        </tr>
                        <tr>
                            <td>Matrix multiply (1024√ó1024)</td>
                            <td>500ms</td>
                            <td>5ms</td>
                            <td>100√ó</td>
                        </tr>
                        <tr>
                            <td>FFT (2048 bins)</td>
                            <td>300ms</td>
                            <td>0.5ms</td>
                            <td>600√ó</td>
                        </tr>
                        <tr>
                            <td>Convolution (order 40)</td>
                            <td>200ms</td>
                            <td>1ms</td>
                            <td>200√ó</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Backend Comparison</h3>
                <table class="performance-table">
                    <thead>
                        <tr>
                            <th>Workload</th>
                            <th>JNI (Native)</th>
                            <th>OpenCL (GPU)</th>
                            <th>Metal (M1)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Vector Add (1M elements)</td>
                            <td>2ms</td>
                            <td>0.1ms</td>
                            <td>0.08ms</td>
                        </tr>
                        <tr>
                            <td>Matrix Multiply (1024√ó1024)</td>
                            <td>800ms</td>
                            <td>10ms</td>
                            <td>6ms</td>
                        </tr>
                        <tr>
                            <td>Element-wise ops (10M)</td>
                            <td>20ms</td>
                            <td>0.5ms</td>
                            <td>0.3ms</td>
                        </tr>
                        <tr>
                            <td>Compilation time</td>
                            <td>50ms</td>
                            <td>200ms</td>
                            <td>150ms</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Memory Transfer Overhead</h3>
                <ul>
                    <li><strong>CPU ‚Üí GPU (1MB):</strong> ~0.5ms (PCIe) or ~0.1ms (Unified Memory)</li>
                    <li><strong>GPU ‚Üí CPU (1MB):</strong> ~1ms (PCIe) or ~0.1ms (Unified Memory)</li>
                    <li><strong>Aggregation Impact:</strong> 3 small arguments ‚Üí 1 large: 3√ó faster transfer</li>
                    <li><strong>Zero-Copy Views:</strong> No transfer, just pointer adjustment</li>
                </ul>

                <h3>Best Practices</h3>
                <ol>
                    <li><strong>Cache kernels with instruct():</strong> First call compiles, subsequent calls reuse</li>
                    <li><strong>Use <a href="../apidocs/org/almostrealism/hardware/PassThroughProducer.html">PassThroughProducer</a> for dynamic inputs:</strong> Compile once, evaluate many times</li>
                    <li><strong>Minimize CPU‚ÜîGPU transfers:</strong> Compose operations on GPU via <a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a></li>
                    <li><strong>Batch operations:</strong> Process arrays instead of scalars</li>
                    <li><strong>Use <a href="../apidocs/org/almostrealism/hardware/MemoryBank.html">MemoryBank</a> for collections:</strong> Single transfer for entire collection</li>
                    <li><strong>Choose appropriate backend:</strong> Metal for Apple Silicon, OpenCL for cross-platform, JNI for CPU</li>
                    <li><strong>Profile before optimizing:</strong> Use <a href="../apidocs/org/almostrealism/hardware/OperationProfile.html">OperationProfile</a> to measure actual bottlenecks</li>
                </ol>
            </section>

            <section id="examples">
                <h2>Usage Examples</h2>

                <h3>Environment Setup</h3>
                <pre><code class="language-bash"># Required: Set these before running any Almost Realism code
export AR_HARDWARE_LIBS=/tmp/ar_libs/
export AR_HARDWARE_DRIVER=native  # or: cl, mtl, gpu, cpu, *

# Optional: Precision and memory
export AR_HARDWARE_PRECISION=FP32  # or FP64
export AR_HARDWARE_MEMORY_SCALE=4  # 1GB (default)</code></pre>

                <h3>Basic Hardware-Accelerated Computation</h3>
                <pre><code class="language-java">import org.almostrealism.hardware.<a href="../apidocs/org/almostrealism/hardware/HardwareFeatures.html">HardwareFeatures</a>;
import org.almostrealism.collect.<a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a>;
import io.almostrealism.relation.<a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a>;

public class Example implements <a href="../apidocs/org/almostrealism/hardware/HardwareFeatures.html">HardwareFeatures</a> {
    public void run() {
        // Create data
        <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> a = new <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><>(1000);
        <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> b = new <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><>(1000);

        // Build computation (NOT executed yet)
        <a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> result = multiply(p(a), p(b));

        // Compile to hardware kernel and execute
        <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> output = result.get().evaluate();
    }
}</code></pre>

                <h3>Kernel Reuse with <a href="../apidocs/org/almostrealism/hardware/PassThroughProducer.html">PassThroughProducer</a></h3>
                <pre><code class="language-java">public class FilterProcessor implements <a href="../apidocs/org/almostrealism/hardware/HardwareFeatures.html">HardwareFeatures</a> {
    private <a href="../apidocs/io/almostrealism/relation/Evaluable.html">Evaluable</a><?> cachedFilter;

    public void setup() {
        // Create filter with dynamic input (argument 0)
        <a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> input = v(shape(1000), 0);
        <a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> filtered = lowPass(input, c(5000.0), 44100, 40);

        // Compile once
        cachedFilter = filtered.get();
    }

    public void process(<a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> data) {
        // Reuse compiled kernel with different data
        <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> result = (<a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?>)cachedFilter.evaluate(data);
        // No recompilation overhead!
    }
}</code></pre>

                <h3><a href="../apidocs/org/almostrealism/hardware/MemoryBank.html">MemoryBank</a> for Batch Operations</h3>
                <pre><code class="language-java">// Single allocation for 1000 vectors
<a href="../apidocs/org/almostrealism/hardware/MemoryBank.html">MemoryBank</a><<a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?>> vectors =
    new <a href="../apidocs/org/almostrealism/hardware/MemoryBankAdapter.html">MemoryBankAdapter</a><>(1000, 3, ...);

// Populate
for (int i = 0; i < 1000; i++) {
    vectors.get(i).setMem(0, i);
    vectors.get(i).setMem(1, i * 2);
    vectors.get(i).setMem(2, i * 3);
}

// Transfer entire bank to GPU as single operation
<a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> p = cp(vectors);
<a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> result = operation.get().evaluate();
// Much faster than 1000 individual transfers!</code></pre>

                <h3><a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a> Composition</h3>
                <pre><code class="language-java">// Compose multiple operations into single kernel
<a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a> pipeline = new <a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a>("Audio Pipeline");
pipeline.add(normalize);
pipeline.add(lowPassFilter);
pipeline.add(gainAdjust);

// If all operations are <a href="../apidocs/io/almostrealism/code/Computation.html">Computation</a>s, compiles to single kernel
Runnable compiled = pipeline.get();
compiled.run();  // Executes entire pipeline on GPU</code></pre>

                <h3>Backend-Specific Requirements</h3>
                <pre><code class="language-java"><a href="../apidocs/org/almostrealism/hardware/DefaultComputer.html">DefaultComputer</a> computer = <a href="../apidocs/org/almostrealism/hardware/Hardware.html">Hardware</a>.getLocalHardware().getComputer();

// Force GPU execution
computer.pushRequirements(List.of(<a href="../apidocs/org/almostrealism/hardware/ComputeRequirement.html">ComputeRequirement</a>.GPU));
try {
    heavyComputation.get().run();  // Runs on GPU
} finally {
    computer.popRequirements();
}

// Or set per-operation
<a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a> gpuOps = new <a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a>();
gpuOps.setComputeRequirements(List.of(<a href="../apidocs/org/almostrealism/hardware/ComputeRequirement.html">ComputeRequirement</a>.GPU));
gpuOps.add(matrixMultiply);
gpuOps.get().run();  // GPU only</code></pre>

                <h3>Instruction Caching Pattern</h3>
                <pre><code class="language-java">public class CachedOperations implements <a href="../apidocs/org/almostrealism/hardware/HardwareFeatures.html">HardwareFeatures</a> {
    public <a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> scale(<a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> input, double factor) {
        // Cached by signature: "scale_2x" + hash(input shape)
        return instruct("scale",
            args -> multiply(args[0], c(factor)),
            input
        );
    }

    public void demo() {
        <a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> data1 = p(new <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><>(1000));
        <a href="../apidocs/io/almostrealism/relation/Producer.html">Producer</a><?> data2 = p(new <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><>(1000));

        // First call: compiles
        scale(data1, 2.0).get().evaluate();

        // Second call: reuses cached kernel
        scale(data2, 2.0).get().evaluate();  // Fast!
    }
}</code></pre>

                <h3>Zero-Copy Memory Views</h3>
                <pre><code class="language-java">// Original 10000-element collection
<a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><?> original = new <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">PackedCollection</a><>(10000);

// Zero-copy view of elements 100-200
<a href="../apidocs/org/almostrealism/hardware/MemoryData.html">MemoryData</a> view = new <a href="../apidocs/org/almostrealism/hardware/mem/Bytes.html">Bytes</a>(100, original, 100);

// Modifications to view affect original
view.setMem(0, 42.0);
assert original.toDouble(100) == 42.0;  // Same memory!

// No data copied, just pointer arithmetic</code></pre>

                <h3>Multi-Backend Fallback</h3>
                <pre><code class="language-bash"># Try Metal first, fallback to OpenCL, then Native
export AR_HARDWARE_DRIVER=mtl,cl,native

# Auto-select best available
export AR_HARDWARE_DRIVER=*</code></pre>

                <h3>Metal Backend: Buffer Creation and Data Transfer</h3>
                <pre><code class="language-java">import org.almostrealism.hardware.metal.*;

// Get Metal device (default GPU)
<a href="../apidocs/org/almostrealism/hardware/metal/MTLDevice.html">MTLDevice</a> device = <a href="../apidocs/org/almostrealism/hardware/metal/MTL.html">MTL</a>.createSystemDefaultDevice();

// Create buffer with FP32 precision (1000 elements)
<a href="../apidocs/org/almostrealism/hardware/metal/MTLBuffer.html">MTLBuffer</a> buffer = device.newBuffer32(1000);

// Write data to buffer
float[] data = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f};
buffer.setContents(data);

// Read data back
float[] result = buffer.getContents32(5);

// Release when done
buffer.release();</code></pre>

                <h3>Metal Backend: Command Encoding Workflow</h3>
                <pre><code class="language-java">// Compile MSL kernel source
String mslSource = """
    [[kernel]] void vectorAdd(
        device float* a [[buffer(0)]],
        device float* b [[buffer(1)]],
        device float* result [[buffer(2)]],
        uint id [[thread_position_in_grid]])
    {
        result[id] = a[id] + b[id];
    }
    """;

<a href="../apidocs/org/almostrealism/hardware/metal/MetalProgram.html">MetalProgram</a> program = <a href="../apidocs/org/almostrealism/hardware/metal/MetalProgram.html">MetalProgram</a>.create(context, metadata, "vectorAdd", mslSource);
program.compile();

// Create command queue
<a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandQueue.html">MTLCommandQueue</a> queue = device.commandQueue();

// Create command buffer (one-time use)
<a href="../apidocs/org/almostrealism/hardware/metal/MTLCommandBuffer.html">MTLCommandBuffer</a> cmdBuffer = queue.commandBuffer();

// Begin encoding compute commands
<a href="../apidocs/org/almostrealism/hardware/metal/MTLComputeCommandEncoder.html">MTLComputeCommandEncoder</a> encoder = cmdBuffer.encoder();

// Set pipeline state (compiled kernel)
encoder.setComputePipelineState(program.getPipeline());

// Bind argument buffers at indices 0, 1, 2
encoder.setBuffer(bufferA, 0, 0);
encoder.setBuffer(bufferB, 0, 1);
encoder.setBuffer(bufferResult, 0, 2);

// Dispatch 1024 threads total, 32 threads per threadgroup
int totalThreads = 1024;
int threadsPerGroup = 32;
encoder.dispatchThreads(totalThreads, 1, 1, threadsPerGroup, 1, 1);

// Finalize encoding
encoder.endEncoding();

// Submit to GPU
cmdBuffer.commit();

// Wait for completion
cmdBuffer.waitUntilCompleted();</code></pre>

                <h3>Metal Backend: High-Level Usage via <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a></h3>
                <pre><code class="language-java">// High-level wrapper handles command encoding automatically
<a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a> op = new <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a>(context, program, "vectorAdd", 3);

// Prepare arguments (typically PackedCollections)
<a href="../apidocs/org/almostrealism/hardware/MemoryData.html">MemoryData</a>[] args = {dataA, dataB, resultData};

// Execute kernel (encoding/dispatch/sync handled internally)
op.accept(args);

// Thread-safe: Each thread gets its own MetalOperator instance via MetalOperatorMap
<a href="../apidocs/org/almostrealism/hardware/metal/MetalOperatorMap.html">MetalOperatorMap</a> map = new <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperatorMap.html">MetalOperatorMap</a>(context, metadata, "vectorAdd", mslSource);
<a href="../apidocs/org/almostrealism/hardware/metal/MetalOperator.html">MetalOperator</a> threadLocalOp = map.get("vectorAdd", 3);
threadLocalOp.accept(args);</code></pre>

                <h3>Metal Backend: Threadgroup Sizing</h3>
                <pre><code class="language-java">// Query optimal threadgroup size
<a href="../apidocs/org/almostrealism/hardware/metal/MTLComputePipelineState.html">MTLComputePipelineState</a> pipeline = program.getPipeline();
int simdWidth = pipeline.threadExecutionWidth();  // Typically 32
int maxThreads = pipeline.maxTotalThreadsPerThreadgroup();  // Typically 1024

// Use SIMD width for optimal performance
int threadsPerGroup = simdWidth;
int totalThreads = dataSize;

// Dispatch with automatic threadgroup calculation
encoder.dispatchThreads(
    totalThreads, 1, 1,      // Total threads (X, Y, Z)
    threadsPerGroup, 1, 1     // Threads per threadgroup (X, Y, Z)
);

// Or explicitly control threadgroup count
int numGroups = (totalThreads + threadsPerGroup - 1) / threadsPerGroup;
encoder.dispatchThreadgroups(
    threadsPerGroup, 1, 1,    // Threads per group
    numGroups, 1, 1           // Number of groups
);</code></pre>
            </section>

            <section id="troubleshooting">
                <h2>Troubleshooting</h2>

                <h3>Common Issues</h3>

                <div class="warning">
                    <strong>Error:</strong> <code>NoClassDefFoundError: Could not initialize class <a href="../apidocs/org/almostrealism/collect/PackedCollection.html">org.almostrealism.collect.PackedCollection</a></code><br>
                    <strong>Cause:</strong> Missing AR_HARDWARE_LIBS or AR_HARDWARE_DRIVER environment variables<br>
                    <strong>Solution:</strong>
                    <pre>export AR_HARDWARE_LIBS=/tmp/ar_libs/
export AR_HARDWARE_DRIVER=native</pre>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> Slow first execution<br>
                    <strong>Expected:</strong> First call takes 100-1000ms (kernel compilation), subsequent calls take 1-10ms<br>
                    <strong>Solution:</strong> Use instruct() pattern or <a href="../apidocs/org/almostrealism/hardware/PassThroughProducer.html">PassThroughProducer</a> to amortize compilation cost
                </div>

                <div class="warning">
                    <strong>Issue:</strong> <a href="../apidocs/org/almostrealism/hardware/OperationList.html">OperationList</a> not compiling to single kernel<br>
                    <strong>Cause:</strong> Mixed <a href="../apidocs/io/almostrealism/code/Computation.html">Computation</a> and non-<a href="../apidocs/io/almostrealism/code/Computation.html">Computation</a> operations, or depth &gt; maxDepth<br>
                    <strong>Check:</strong> <code>boolean canCompile = ops.isComputation();</code><br>
                    <strong>Solution:</strong> Separate compiled and non-compiled operations, or flatten deep lists
                </div>

                <div class="warning">
                    <strong>Error:</strong> <code><a href="../apidocs/org/almostrealism/hardware/HardwareException.html">HardwareException</a>: Memory Max Reached</code><br>
                    <strong>Cause:</strong> Insufficient GPU memory allocation<br>
                    <strong>Solution:</strong>
                    <pre>export AR_HARDWARE_MEMORY_SCALE=6  # 4GB
# or use host memory
export AR_HARDWARE_MEMORY_LOCATION=host</pre>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> Metal backend not working on macOS<br>
                    <strong>Requirements:</strong> macOS 10.13+, Metal-capable GPU<br>
                    <strong>Check:</strong> Verify with <code>system_profiler SPDisplaysDataType | grep Metal</code><br>
                    <strong>Fallback:</strong> Use <code>AR_HARDWARE_DRIVER=cl,native</code>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> OpenCL compilation errors<br>
                    <strong>Debug:</strong> Enable kernel logging: <code><a href="../apidocs/org/almostrealism/hardware/HardwareOperator.html">HardwareOperator</a>.enableKernelLog = true;</code><br>
                    <strong>Check:</strong> Generated kernels in results/instruction_set_*.cl<br>
                    <strong>Common causes:</strong> Precision mismatch (FP64 on FP32-only GPU), unsupported operations
                </div>

                <h3>Metal-Specific Issues</h3>

                <div class="warning">
                    <strong>Issue:</strong> Metal backend initialization failure<br>
                    <strong>Requirements:</strong>
                    <ul>
                        <li>macOS 10.13+ or iOS 11+</li>
                        <li>Metal-capable GPU (check: <code>system_profiler SPDisplaysDataType | grep Metal</code>)</li>
                        <li>Native library libMTL.dylib accessible on java.library.path</li>
                    </ul>
                    <strong>Solution:</strong> Use <code>AR_HARDWARE_DRIVER=cl,native</code> for fallback
                </div>

                <div class="warning">
                    <strong>Issue:</strong> MSL compilation errors<br>
                    <strong>Debug:</strong> Enable Metal operator logging:
                    <pre>MetalOperator.enableLog = true;
MetalOperator.enableVerboseLog = true;  // Prints MSL source</pre>
                    <strong>Check:</strong> Look for Metal-specific syntax issues:
                    <ul>
                        <li>Address space qualifiers (<code>device</code>, <code>thread</code>, <code>constant</code>)</li>
                        <li>Kernel attributes (<code>[[kernel]]</code>, <code>[[buffer(N)]]</code>)</li>
                        <li>Unsupported C++ features (Metal uses subset of C++14)</li>
                    </ul>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> Threadgroup size errors<br>
                    <strong>Error message:</strong> <code>Compute Function exceeds available resources</code><br>
                    <strong>Cause:</strong> Threadgroup size exceeds device limits<br>
                    <strong>Solution:</strong> Query device capabilities:
                    <pre>MTLComputePipelineState pipeline = program.getPipeline();
int maxThreads = pipeline.maxTotalThreadsPerThreadgroup();  // Use this limit
int simdWidth = pipeline.threadExecutionWidth();  // Optimal size</pre>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> Buffer allocation failures<br>
                    <strong>Error:</strong> <code>OutOfMemoryError</code> or null MTLBuffer<br>
                    <strong>Causes:</strong>
                    <ul>
                        <li>Insufficient GPU memory</li>
                        <li>Too many active buffers</li>
                        <li>Buffer not released after use</li>
                    </ul>
                    <strong>Solution:</strong>
                    <pre>// Always release buffers
buffer.release();

// Or use try-with-resources pattern if supported
// Check AR_HARDWARE_MEMORY_SCALE setting</pre>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> Incorrect kernel results<br>
                    <strong>Common causes:</strong>
                    <ul>
                        <li><strong>Missing synchronization:</strong> Call <code>cmdBuffer.waitUntilCompleted()</code></li>
                        <li><strong>Race conditions:</strong> Use thread-local operators via <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperatorMap.html">MetalOperatorMap</a></li>
                        <li><strong>Precision issues:</strong> FP16 has limited range (¬±65504) and precision (~3 decimal digits)</li>
                        <li><strong>Threadgroup sizing:</strong> Ensure total threads ‚â• data size</li>
                    </ul>
                </div>

                <div class="warning">
                    <strong>Issue:</strong> Performance slower than expected<br>
                    <strong>Check:</strong>
                    <ul>
                        <li><strong>Unified memory advantage:</strong> On Apple Silicon, shared storage mode avoids transfers</li>
                        <li><strong>SIMD width alignment:</strong> Use threadgroup size = multiple of SIMD width (32)</li>
                        <li><strong>Kernel recompilation:</strong> Enable <a href="../apidocs/org/almostrealism/hardware/metal/MetalOperatorMap.html">MetalOperatorMap</a> caching</li>
                        <li><strong>Excessive synchronization:</strong> Minimize waitUntilCompleted() calls</li>
                    </ul>
                </div>
            </section>

            <section id="resources">
                <h2>Additional Resources</h2>
                <ul>
                    <li><a href="../../hardware/README.md">Hardware Module README</a> - Comprehensive module documentation</li>
                    <li><a href="../apidocs/org/almostrealism/hardware/package-summary.html">JavaDoc API</a> - Complete API reference</li>
                    <li><a href="../tutorials/">Framework Tutorials</a> - Step-by-step guides</li>
                </ul>

                <h3>Backend-Specific Documentation</h3>
                <ul>
                    <li><a href="../apidocs/org/almostrealism/hardware/cl/package-summary.html">OpenCL Backend (cl package)</a></li>
                    <li><a href="../apidocs/org/almostrealism/hardware/metal/package-summary.html">Metal Backend (metal package)</a></li>
                    <li><a href="../apidocs/org/almostrealism/hardware/jni/package-summary.html">JNI Backend (jni package)</a></li>
                    <li><a href="../apidocs/org/almostrealism/hardware/mem/package-summary.html">Memory Management (mem package)</a></li>
                </ul>
            </section>
        </main>

        <footer>
            <p><a href="../index.html">‚Üê Back to Framework Documentation</a></p>
            <p>&copy; 2025 Michael Murray. Licensed under the Apache License, Version 2.0.</p>
        </footer>
    </div>

    <script src="../js/docs.js"></script>
</body>
</html>
